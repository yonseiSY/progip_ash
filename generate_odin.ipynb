{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78314e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Copyright (c) 2017-present, Facebook, Inc.\n",
    "# All rights reserved.\n",
    "#\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "#\n",
    "\n",
    "\"\"\"\n",
    "Created on Sat Sep 19 20:55:56 2015\n",
    "\n",
    "@author: liangshiyu\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from scipy import misc\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d483e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e436a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def odin(model=None, id_testloader=None, out_testloader=None, magnitude=0.0014, temperature=1000, std=(0,0,0), file_path=''):\n",
    "    g1 = open(f\"{file_path}/confidence_Our_In.txt\", 'w')\n",
    "    g2 = open(f\"{file_path}/confidence_Our_Out.txt\", 'w')\n",
    "    \n",
    "    print(\"Processing in-distribution images\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "########################################In-distribution###########################################\n",
    "    for j, data in enumerate(id_testloader):\n",
    "        images, _ = data\n",
    "        batch_size = images.size(0)\n",
    "        \n",
    "        inputs = images.to(device).requires_grad_()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Calculating the confidence of the output, no perturbation added here, no temperature scaling used\n",
    "        nnOutputs = outputs.detach().cpu().numpy()\n",
    "        nnOutputs = np.exp(nnOutputs - np.max(nnOutputs, axis=1, keepdims=True))\n",
    "        nnOutputs = nnOutputs / np.sum(nnOutputs, axis=1, keepdims=True)\n",
    "        \n",
    "        # Using temperature scaling\n",
    "        outputs = outputs / temperature\n",
    "\t\n",
    "        # Calculating the perturbation we need to add, that is,\n",
    "        # the sign of gradient of cross entropy loss w.r.t. input\n",
    "        maxIndexTemp = torch.argmax(outputs, dim=1)\n",
    "        labels = maxIndexTemp.to(device)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Normalizing the gradient to binary in {0, 1}\n",
    "        gradient = torch.ge(inputs.grad.data, 0)\n",
    "        gradient = (gradient.float() - 0.5) * 2\n",
    "        \n",
    "        # Normalizing the gradient to the same space of image\n",
    "        gradient[0][0] = (gradient[0][0]) / std[0]\n",
    "        gradient[0][1] = (gradient[0][1]) / std[1]\n",
    "        gradient[0][2] = (gradient[0][2]) / std[2]\n",
    "        \n",
    "        # Adding small perturbations to images\n",
    "        tempInputs = torch.add(inputs.detach(), gradient, alpha=-magnitude)\n",
    "        outputs = model(tempInputs)\n",
    "        outputs = outputs / temperature\n",
    "        \n",
    "        # Calculating the confidence after adding perturbations\n",
    "        nnOutputs = outputs.data.cpu().numpy()\n",
    "        nnOutputs = np.exp(nnOutputs - np.max(nnOutputs, axis=1, keepdims=True))\n",
    "        nnOutputs = nnOutputs / np.sum(nnOutputs, axis=1, keepdims=True)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            g1.write(\"{}, {}, {}\\n\".format(temperature, magnitude, np.max(nnOutputs[i])))\n",
    "            \n",
    "        images.grad = None\n",
    "        \n",
    "    print(\"Processing out-of-distribution images\")\n",
    "###################################Out-of-Distributions#####################################\n",
    "    for j, data in enumerate(out_testloader):\n",
    "        images, _ = data\n",
    "        batch_size = images.size(0)\n",
    "    \n",
    "        inputs = images.to(device).requires_grad_()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Calculating the confidence of the output, no perturbation added here\n",
    "        nnOutputs = outputs.detach().cpu().numpy()\n",
    "        nnOutputs = np.exp(nnOutputs - np.max(nnOutputs, axis=1, keepdims=True))\n",
    "        nnOutputs = nnOutputs / np.sum(nnOutputs, axis=1, keepdims=True)\n",
    "        \n",
    "        # Using temperature scaling\n",
    "        outputs = outputs / temperature  \n",
    "  \n",
    "        # Calculating the perturbation we need to add, that is,\n",
    "        # the sign of gradient of cross entropy loss w.r.t. input\n",
    "        maxIndexTemp = torch.argmax(outputs, dim=1)\n",
    "        labels = maxIndexTemp.to(device)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Normalizing the gradient to binary in {0, 1}\n",
    "        gradient = torch.ge(inputs.grad.data, 0)\n",
    "        gradient = (gradient.float() - 0.5) * 2\n",
    "        \n",
    "        # Normalizing the gradient to the same space of image\n",
    "        gradient[0][0] = (gradient[0][0]) / std[0]\n",
    "        gradient[0][1] = (gradient[0][1]) / std[1]\n",
    "        gradient[0][2] = (gradient[0][2]) / std[2]\n",
    "        \n",
    "        # Adding small perturbations to images\n",
    "        tempInputs = torch.add(inputs.detach(), gradient, alpha=-magnitude)\n",
    "        outputs = model(tempInputs)\n",
    "        outputs = outputs / temperature\n",
    "        \n",
    "        # Calculating the confidence after adding perturbations\n",
    "        nnOutputs = outputs.data.cpu().numpy()\n",
    "        nnOutputs = np.exp(nnOutputs - np.max(nnOutputs, axis=1, keepdims=True))\n",
    "        nnOutputs = nnOutputs / np.sum(nnOutputs, axis=1, keepdims=True)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            g2.write(\"{}, {}, {}\\n\".format(temperature, magnitude, np.max(nnOutputs[i])))\n",
    "            \n",
    "        images.grad = None\n",
    "        \n",
    "    g1.close()\n",
    "    g2.close()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16fadb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fault_injector as fi\n",
    "\n",
    "def odin_fi(model=None, id_testloader=None, out_testloader=None, magnitude=0.0014, temperature=1000, std=(0,0,0), file_path='', \n",
    "            first_forward_fi=False, backward_fi=False, second_forward_fi=False, bit_positions=[], flip_ratio=0, layer_name=''):    \n",
    "    g1 = open(f\"{file_path}/confidence_Our_In_fi.txt\", 'w')\n",
    "    g2 = open(f\"{file_path}/confidence_Our_Out_fi.txt\", 'w')\n",
    "    \n",
    "    print(\"Processing in-distribution images\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "########################################In-distribution###########################################\n",
    "    for j, data in enumerate(id_testloader):\n",
    "        images, _ = data\n",
    "        batch_size = images.size(0)\n",
    "        \n",
    "        inputs = images.to(device).requires_grad_()\n",
    "        \n",
    "        layer = dict(model.named_modules())[layer_name]\n",
    "        \n",
    "        # --- 첫 번째 forward에서 activation bit-flip ---\n",
    "        flip_log = []\n",
    "        \n",
    "        if first_forward_fi:\n",
    "            hook_handle = layer.register_forward_hook(\n",
    "                fi.get_activation_bitflip_hook(bit_positions, flip_ratio, flip_log)\n",
    "            )        \n",
    "            \n",
    "        if backward_fi:\n",
    "            activation_holder = {'act': None}\n",
    "            def grab_forward(module, inp, out):\n",
    "                activation_holder['act'] = out\n",
    "            act_handle = layer.register_forward_hook(grab_forward)            \n",
    "            \n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        if first_forward_fi:\n",
    "            hook_handle.remove()\n",
    "            # 로그 출력\n",
    "            # print(f\"[ID][Layer {layer_name}] 총 {len(flip_log)} flips:\")\n",
    "            # for idx, orig, flipped in flip_log[:5]:\n",
    "            #     print(f\"  idx={idx}, {orig:.6f} → {flipped:.6f}\")      \n",
    "                \n",
    "        if backward_fi:\n",
    "            act_handle.remove()\n",
    "            activation = activation_holder['act']                      \n",
    "        \n",
    "        # Calculating the confidence of the output, no perturbation added here, no temperature scaling used\n",
    "        nnOutputs = outputs.detach().cpu().numpy()\n",
    "        nnOutputs = np.exp(nnOutputs - np.max(nnOutputs, axis=1, keepdims=True))\n",
    "        nnOutputs = nnOutputs / np.sum(nnOutputs, axis=1, keepdims=True)\n",
    "        \n",
    "        # Using temperature scaling\n",
    "        outputs = outputs / temperature\n",
    "\t\n",
    "        # Calculating the perturbation we need to add, that is,\n",
    "        # the sign of gradient of cross entropy loss w.r.t. input\n",
    "        maxIndexTemp = torch.argmax(outputs, dim=1)\n",
    "        labels = maxIndexTemp.to(device)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # 2) backward gradient bit-flip\n",
    "        flip_log = []\n",
    "        \n",
    "        if backward_fi:\n",
    "            grad_handle = activation.register_hook(\n",
    "                fi.get_gradient_bitflip_hook(bit_positions, flip_ratio, flip_log)\n",
    "            )\n",
    "            \n",
    "        loss.backward()\n",
    "        \n",
    "        if backward_fi:\n",
    "            grad_handle.remove()\n",
    "            # print(f\"[ID][Layer {layer_name} – backward] 총 {len(flip_log)} flips:\")\n",
    "            # for idx, orig, flipped in flip_log[:5]:\n",
    "            #     print(f\"  idx={idx}, {orig} → {flipped}\")   \n",
    "                    \n",
    "        # Normalizing the gradient to binary in {0, 1}\n",
    "        gradient = torch.ge(inputs.grad.data, 0)\n",
    "        gradient = (gradient.float() - 0.5) * 2\n",
    "        \n",
    "        # Normalizing the gradient to the same space of image\n",
    "        gradient[0][0] = (gradient[0][0]) / std[0]\n",
    "        gradient[0][1] = (gradient[0][1]) / std[1]\n",
    "        gradient[0][2] = (gradient[0][2]) / std[2]\n",
    "        \n",
    "        # Adding small perturbations to images\n",
    "        tempInputs = torch.add(inputs.detach(), gradient, alpha=-magnitude)\n",
    "        \n",
    "        # --- 두 번째 forward에서 activation bit-flip ---\n",
    "        flip_log2 = []  # ① 두 번째 forward 전용 로그 리스트\n",
    "        if second_forward_fi:\n",
    "            hook_handle2 = layer.register_forward_hook(\n",
    "                fi.get_activation_bitflip_hook(bit_positions, flip_ratio, flip_log2)\n",
    "            )        \n",
    "\n",
    "        outputs = model(tempInputs)\n",
    "\n",
    "        if second_forward_fi:\n",
    "            hook_handle2.remove()    \n",
    "            # ② 두 번째 forward 로그 출력\n",
    "            # print(f\"[ID][Layer {layer_name} – 2nd forward] 총 {len(flip_log2)} flips:\")\n",
    "            # for idx, orig, flipped in flip_log2[:5]:\n",
    "            #     print(f\"  idx={idx}, {orig:.6f} → {flipped:.6f}\")                \n",
    "\n",
    "        outputs = outputs / temperature\n",
    "        \n",
    "        # Calculating the confidence after adding perturbations\n",
    "        nnOutputs = outputs.data.cpu().numpy()\n",
    "        nnOutputs = np.exp(nnOutputs - np.max(nnOutputs, axis=1, keepdims=True))\n",
    "        nnOutputs = nnOutputs / np.sum(nnOutputs, axis=1, keepdims=True)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            g1.write(\"{}, {}, {}\\n\".format(temperature, magnitude, np.max(nnOutputs[i])))\n",
    "            \n",
    "        images.grad = None\n",
    "        \n",
    "    print(\"Processing out-of-distribution images\")\n",
    "###################################Out-of-Distributions#####################################\n",
    "    for j, data in enumerate(out_testloader):\n",
    "        images, _ = data\n",
    "        batch_size = images.size(0)\n",
    "    \n",
    "        inputs = images.to(device).requires_grad_()\n",
    "        \n",
    "        layer = dict(model.named_modules())[layer_name]\n",
    "\n",
    "        # --- 첫 번째 forward에서 activation bit-flip ---\n",
    "        flip_log = []   \n",
    "        \n",
    "        if first_forward_fi:\n",
    "            hook_handle = layer.register_forward_hook(\n",
    "                fi.get_activation_bitflip_hook(bit_positions, flip_ratio, flip_log)\n",
    "            )        \n",
    "            \n",
    "        if backward_fi:\n",
    "            activation_holder = {'act': None}\n",
    "            def grab_forward(module, inp, out):\n",
    "                activation_holder['act'] = out\n",
    "            act_handle = layer.register_forward_hook(grab_forward)                   \n",
    "            \n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        if first_forward_fi:\n",
    "            hook_handle.remove()\n",
    "            # 로그 출력\n",
    "            # print(f\"[OOD][Layer {layer_name}] 총 {len(flip_log)} flips:\")\n",
    "            # for idx, orig, flipped in flip_log[:5]:\n",
    "            #     print(f\"  idx={idx}, {orig:.6f} → {flipped:.6f}\")                  \n",
    "            \n",
    "        if backward_fi:\n",
    "            act_handle.remove()\n",
    "            activation = activation_holder['act']                 \n",
    "        \n",
    "        # Calculating the confidence of the output, no perturbation added here\n",
    "        nnOutputs = outputs.detach().cpu().numpy()\n",
    "        nnOutputs = np.exp(nnOutputs - np.max(nnOutputs, axis=1, keepdims=True))\n",
    "        nnOutputs = nnOutputs / np.sum(nnOutputs, axis=1, keepdims=True)\n",
    "        \n",
    "        # Using temperature scaling\n",
    "        outputs = outputs / temperature  \n",
    "  \n",
    "        # Calculating the perturbation we need to add, that is,\n",
    "        # the sign of gradient of cross entropy loss w.r.t. input\n",
    "        maxIndexTemp = torch.argmax(outputs, dim=1)\n",
    "        labels = maxIndexTemp.to(device)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # 2) backward gradient bit-flip\n",
    "        flip_log = []\n",
    "        \n",
    "        if backward_fi:\n",
    "            grad_handle = activation.register_hook(\n",
    "                fi.get_gradient_bitflip_hook(bit_positions, flip_ratio, flip_log)\n",
    "            )\n",
    "            \n",
    "        loss.backward()\n",
    "        \n",
    "        if backward_fi:\n",
    "            grad_handle.remove()\n",
    "            # print(f\"[OOD][Layer {layer_name} – backward] 총 {len(flip_log)} flips:\")\n",
    "            # for idx, orig, flipped in flip_log[:5]:\n",
    "            #     print(f\"  idx={idx}, {orig:.6f} → {flipped:.6f}\")     \n",
    "    \n",
    "        # Normalizing the gradient to binary in {0, 1}\n",
    "        gradient = torch.ge(inputs.grad.data, 0)\n",
    "        gradient = (gradient.float() - 0.5) * 2\n",
    "        \n",
    "        # Normalizing the gradient to the same space of image\n",
    "        gradient[0][0] = (gradient[0][0]) / std[0]\n",
    "        gradient[0][1] = (gradient[0][1]) / std[1]\n",
    "        gradient[0][2] = (gradient[0][2]) / std[2]\n",
    "        \n",
    "        # Adding small perturbations to images\n",
    "        tempInputs = torch.add(inputs.detach(), gradient, alpha=-magnitude)\n",
    "\n",
    "        outputs = model(tempInputs)\n",
    "\n",
    "        # --- 두 번째 forward에서 activation bit-flip ---\n",
    "        flip_log2 = []          \n",
    "        if second_forward_fi:\n",
    "            hook_handle2 = layer.register_forward_hook(\n",
    "                fi.get_activation_bitflip_hook(bit_positions, flip_ratio, flip_log2)\n",
    "            )        \n",
    "\n",
    "        outputs = model(tempInputs)\n",
    "\n",
    "        if second_forward_fi:\n",
    "            hook_handle2.remove()   \n",
    "            # print(f\"[OOD][Layer {layer_name} – 2nd forward] 총 {len(flip_log2)} flips:\")\n",
    "            # for idx, orig, flipped in flip_log2[:5]:\n",
    "            #     print(f\"  idx={idx}, {orig:.6f} → {flipped:.6f}\") \n",
    "\n",
    "        outputs = outputs / temperature\n",
    "        \n",
    "        # Calculating the confidence after adding perturbations\n",
    "        nnOutputs = outputs.data.cpu().numpy()\n",
    "        nnOutputs = np.exp(nnOutputs - np.max(nnOutputs, axis=1, keepdims=True))\n",
    "        nnOutputs = nnOutputs / np.sum(nnOutputs, axis=1, keepdims=True)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            g2.write(\"{}, {}, {}\\n\".format(temperature, magnitude, np.max(nnOutputs[i])))\n",
    "            \n",
    "        images.grad = None\n",
    "        \n",
    "    g1.close()\n",
    "    g2.close()      \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
