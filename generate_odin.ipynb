{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78314e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Copyright (c) 2017-present, Facebook, Inc.\n",
    "# All rights reserved.\n",
    "#\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "#\n",
    "\n",
    "\"\"\"\n",
    "Created on Sat Sep 19 20:55:56 2015\n",
    "\n",
    "@author: liangshiyu\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from scipy import misc\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d483e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e436a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def odin(model=None, id_testloader=None, out_testloader=None, magnitude=0.0014, temperature=1000, std=(0,0,0), file_path=''):\n",
    "    g1 = open(f\"{file_path}/confidence_Our_In.txt\", 'w')\n",
    "    g2 = open(f\"{file_path}/confidence_Our_Out.txt\", 'w')\n",
    "    \n",
    "    print(\"Processing in-distribution images\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "########################################In-distribution###########################################\n",
    "    for j, data in enumerate(id_testloader):\n",
    "        images, _ = data\n",
    "        batch_size = images.size(0)\n",
    "        \n",
    "        inputs = images.to(device).requires_grad_()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Calculating the confidence of the output, no perturbation added here, no temperature scaling used\n",
    "        nnOutputs = outputs.detach().cpu().numpy()\n",
    "        nnOutputs = np.exp(nnOutputs - np.max(nnOutputs, axis=1, keepdims=True))\n",
    "        nnOutputs = nnOutputs / np.sum(nnOutputs, axis=1, keepdims=True)\n",
    "        \n",
    "        # Using temperature scaling\n",
    "        outputs = outputs / temperature\n",
    "\t\n",
    "        # Calculating the perturbation we need to add, that is,\n",
    "        # the sign of gradient of cross entropy loss w.r.t. input\n",
    "        maxIndexTemp = torch.argmax(outputs, dim=1)\n",
    "        labels = maxIndexTemp.to(device)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Normalizing the gradient to binary in {0, 1}\n",
    "        gradient = torch.ge(inputs.grad.data, 0)\n",
    "        gradient = (gradient.float() - 0.5) * 2\n",
    "        \n",
    "        # Normalizing the gradient to the same space of image\n",
    "        gradient[0][0] = (gradient[0][0]) / std[0]\n",
    "        gradient[0][1] = (gradient[0][1]) / std[1]\n",
    "        gradient[0][2] = (gradient[0][2]) / std[2]\n",
    "        \n",
    "        # Adding small perturbations to images\n",
    "        tempInputs = torch.add(inputs.detach(), gradient, alpha=-magnitude)\n",
    "        outputs = model(tempInputs)\n",
    "        outputs = outputs / temperature\n",
    "        \n",
    "        # Calculating the confidence after adding perturbations\n",
    "        nnOutputs = outputs.data.cpu().numpy()\n",
    "        nnOutputs = np.exp(nnOutputs - np.max(nnOutputs, axis=1, keepdims=True))\n",
    "        nnOutputs = nnOutputs / np.sum(nnOutputs, axis=1, keepdims=True)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            g1.write(\"{}, {}, {}\\n\".format(temperature, magnitude, np.max(nnOutputs[i])))\n",
    "            \n",
    "        images.grad = None\n",
    "        \n",
    "    print(\"Processing out-of-distribution images\")\n",
    "###################################Out-of-Distributions#####################################\n",
    "    for j, data in enumerate(out_testloader):\n",
    "        images, _ = data\n",
    "        batch_size = images.size(0)\n",
    "    \n",
    "        inputs = images.to(device).requires_grad_()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Calculating the confidence of the output, no perturbation added here\n",
    "        nnOutputs = outputs.detach().cpu().numpy()\n",
    "        nnOutputs = np.exp(nnOutputs - np.max(nnOutputs, axis=1, keepdims=True))\n",
    "        nnOutputs = nnOutputs / np.sum(nnOutputs, axis=1, keepdims=True)\n",
    "        \n",
    "        # Using temperature scaling\n",
    "        outputs = outputs / temperature  \n",
    "  \n",
    "        # Calculating the perturbation we need to add, that is,\n",
    "        # the sign of gradient of cross entropy loss w.r.t. input\n",
    "        maxIndexTemp = torch.argmax(outputs, dim=1)\n",
    "        labels = maxIndexTemp.to(device)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Normalizing the gradient to binary in {0, 1}\n",
    "        gradient = torch.ge(inputs.grad.data, 0)\n",
    "        gradient = (gradient.float() - 0.5) * 2\n",
    "        \n",
    "        # Normalizing the gradient to the same space of image\n",
    "        gradient[0][0] = (gradient[0][0]) / std[0]\n",
    "        gradient[0][1] = (gradient[0][1]) / std[1]\n",
    "        gradient[0][2] = (gradient[0][2]) / std[2]\n",
    "        \n",
    "        # Adding small perturbations to images\n",
    "        tempInputs = torch.add(inputs.detach(), gradient, alpha=-magnitude)\n",
    "        outputs = model(tempInputs)\n",
    "        outputs = outputs / temperature\n",
    "        \n",
    "        # Calculating the confidence after adding perturbations\n",
    "        nnOutputs = outputs.data.cpu().numpy()\n",
    "        nnOutputs = np.exp(nnOutputs - np.max(nnOutputs, axis=1, keepdims=True))\n",
    "        nnOutputs = nnOutputs / np.sum(nnOutputs, axis=1, keepdims=True)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            g2.write(\"{}, {}, {}\\n\".format(temperature, magnitude, np.max(nnOutputs[i])))\n",
    "            \n",
    "        images.grad = None\n",
    "        \n",
    "    g1.close()\n",
    "    g2.close()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16fadb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fault_injector as fi\n",
    "\n",
    "def odin_fi(model=None, id_testloader=None, out_testloader=None, magnitude=0.0014, temperature=1000, std=(0,0,0), file_path='', \n",
    "            first_forward_fi=False, backward_fi=False, second_forward_fi=False, bit_lists=[], flip_ratio=0, layer_name='', fs=None):    \n",
    "    g1 = open(f\"{file_path}/confidence_Our_In_fi.txt\", 'w')\n",
    "    g2 = open(f\"{file_path}/confidence_Our_Out_fi.txt\", 'w')\n",
    "    \n",
    "    print(\"Processing in-distribution images\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "########################################In-distribution###########################################\n",
    "    for j, data in enumerate(id_testloader):\n",
    "        images, _ = data\n",
    "        batch_size = images.size(0)\n",
    "        \n",
    "        inputs = images.to(device).requires_grad_()     \n",
    "        \n",
    "        hook_handlers = []\n",
    "        \n",
    "        if first_forward_fi:\n",
    "            hook_handler = fs.onlineMultiBitLayerOutputInjection(model=model, targetLayer=layer_name, NofError=flip_ratio, bit_positions=bit_lists)\n",
    "            hook_handlers.append(hook_handler)\n",
    "            # print(f\"[ID] 첫번째 forward pass - 레이어 {layer_name}의 {str(bit_lists)} 비트에 {flip_ratio} 비율로 결함 주입 설정\")\n",
    "            \n",
    "            if hasattr(fs, '_current_injection_stats'):\n",
    "                stats = fs._current_injection_stats\n",
    "                # print(f\"[ID] 오류 주입된 뉴런 비율: {stats['injected_neurons']}/{stats['total_neurons']} = {stats['ratio']:.4f}\")\n",
    "            \n",
    "        outputs = model(inputs)               \n",
    "        \n",
    "        for handler in hook_handlers:\n",
    "            handler.remove()\n",
    "            \n",
    "        hook_handlers = []\n",
    "        \n",
    "        # Calculating the confidence of the output, no perturbation added here, no temperature scaling used\n",
    "        nnOutputs = outputs.detach().cpu().numpy()\n",
    "        nnOutputs = np.exp(nnOutputs - np.max(nnOutputs, axis=1, keepdims=True))\n",
    "        nnOutputs = nnOutputs / np.sum(nnOutputs, axis=1, keepdims=True)\n",
    "        \n",
    "        # Using temperature scaling\n",
    "        outputs = outputs / temperature\n",
    "\t\n",
    "        # Calculating the perturbation we need to add, that is,\n",
    "        # the sign of gradient of cross entropy loss w.r.t. input\n",
    "        maxIndexTemp = torch.argmax(outputs, dim=1)\n",
    "        labels = maxIndexTemp.to(device)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        if backward_fi:\n",
    "            hook_handler = fs.onlineMultiBitLayerBackwardInjection(model=model, targetLayer=layer_name, NofError=flip_ratio, bit_positions=bit_lists)\n",
    "            hook_handlers.append(hook_handler)\n",
    "            # print(f\"[ID] Backward pass - 레이어 {layer_name}의 {str(bit_lists)} 비트에 {flip_ratio} 비율로 결함 주입 설정\")\n",
    "            \n",
    "            if hasattr(fs, '_current_injection_stats'):\n",
    "                stats = fs._current_injection_stats\n",
    "                # print(f\"[ID] 오류 주입된 뉴런 비율: {stats['injected_neurons']}/{stats['total_neurons']} = {stats['ratio']:.4f}\")\n",
    "            \n",
    "        loss.backward()\n",
    "        \n",
    "        for handler in hook_handlers:\n",
    "            handler.remove()\n",
    "        \n",
    "        hook_handlers = []\n",
    "                    \n",
    "        # Normalizing the gradient to binary in {0, 1}\n",
    "        gradient = torch.ge(inputs.grad.data, 0)\n",
    "        gradient = (gradient.float() - 0.5) * 2\n",
    "        \n",
    "        # Normalizing the gradient to the same space of image\n",
    "        gradient[0][0] = (gradient[0][0]) / std[0]\n",
    "        gradient[0][1] = (gradient[0][1]) / std[1]\n",
    "        gradient[0][2] = (gradient[0][2]) / std[2]\n",
    "        \n",
    "        # Adding small perturbations to images\n",
    "        tempInputs = torch.add(inputs.detach(), gradient, alpha=-magnitude)\n",
    "        \n",
    "        if second_forward_fi:\n",
    "            hook_handler = fs.onlineMultiBitLayerOutputInjection(model=model, targetLayer=layer_name, NofError=flip_ratio, bit_positions=bit_lists)\n",
    "            hook_handlers.append(hook_handler)\n",
    "            # print(f\"[ID] 두번째 forward pass - 레이어 {layer_name}의 {str(bit_lists)} 비트에 {flip_ratio} 비율로 결함 주입 설정\")\n",
    "            \n",
    "            if hasattr(fs, '_current_injection_stats'):\n",
    "                stats = fs._current_injection_stats\n",
    "                # print(f\"[ID] 오류 주입된 뉴런 비율: {stats['injected_neurons']}/{stats['total_neurons']} = {stats['ratio']:.4f}\")            \n",
    "        \n",
    "        outputs = model(tempInputs)      \n",
    "        \n",
    "        for handler in hook_handlers:\n",
    "            handler.remove()       \n",
    "\n",
    "        outputs = outputs / temperature\n",
    "        \n",
    "        # Calculating the confidence after adding perturbations\n",
    "        nnOutputs = outputs.data.cpu().numpy()\n",
    "        nnOutputs = np.exp(nnOutputs - np.max(nnOutputs, axis=1, keepdims=True))\n",
    "        nnOutputs = nnOutputs / np.sum(nnOutputs, axis=1, keepdims=True)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            g1.write(\"{}, {}, {}\\n\".format(temperature, magnitude, np.max(nnOutputs[i])))\n",
    "            \n",
    "        images.grad = None\n",
    "        \n",
    "    print(\"Processing out-of-distribution images\")\n",
    "###################################Out-of-Distributions#####################################\n",
    "    for j, data in enumerate(out_testloader):\n",
    "        images, _ = data\n",
    "        batch_size = images.size(0)\n",
    "    \n",
    "        inputs = images.to(device).requires_grad_()           \n",
    "            \n",
    "        hook_handlers = []\n",
    "        \n",
    "        if first_forward_fi:\n",
    "            hook_handler = fs.onlineMultiBitLayerOutputInjection(model=model, targetLayer=layer_name, NofError=flip_ratio, bit_positions=bit_lists)\n",
    "            hook_handlers.append(hook_handler)\n",
    "            # print(f\"[OOD] 첫번째 forward pass - 레이어 {layer_name}의 {str(bit_lists)} 비트에 {flip_ratio} 비율로 결함 주입 설정\")\n",
    "            \n",
    "            if hasattr(fs, '_current_injection_stats'):\n",
    "                stats = fs._current_injection_stats\n",
    "                # print(f\"[OOD] 오류 주입된 뉴런 비율: {stats['injected_neurons']}/{stats['total_neurons']} = {stats['ratio']:.4f}\")            \n",
    "            \n",
    "        outputs = model(inputs)               \n",
    "        \n",
    "        for handler in hook_handlers:\n",
    "            handler.remove()\n",
    "            \n",
    "        hook_handlers = []\n",
    "        \n",
    "        # Calculating the confidence of the output, no perturbation added here\n",
    "        nnOutputs = outputs.detach().cpu().numpy()\n",
    "        nnOutputs = np.exp(nnOutputs - np.max(nnOutputs, axis=1, keepdims=True))\n",
    "        nnOutputs = nnOutputs / np.sum(nnOutputs, axis=1, keepdims=True)\n",
    "        \n",
    "        # Using temperature scaling\n",
    "        outputs = outputs / temperature  \n",
    "  \n",
    "        # Calculating the perturbation we need to add, that is,\n",
    "        # the sign of gradient of cross entropy loss w.r.t. input\n",
    "        maxIndexTemp = torch.argmax(outputs, dim=1)\n",
    "        labels = maxIndexTemp.to(device)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        if backward_fi:\n",
    "            hook_handler = fs.onlineMultiBitLayerBackwardInjection(model=model, targetLayer=layer_name, NofError=flip_ratio, bit_positions=bit_lists)\n",
    "            hook_handlers.append(hook_handler)\n",
    "            # print(f\"[OOD] Backward pass - 레이어 {layer_name}의 {str(bit_lists)} 비트에 {flip_ratio} 비율로 결함 주입 설정\")\n",
    "            \n",
    "            if hasattr(fs, '_current_injection_stats'):\n",
    "                stats = fs._current_injection_stats\n",
    "                # print(f\"[OOD] 오류 주입된 뉴런 비율: {stats['injected_neurons']}/{stats['total_neurons']} = {stats['ratio']:.4f}\")            \n",
    "            \n",
    "        loss.backward()\n",
    "        \n",
    "        for handler in hook_handlers:\n",
    "            handler.remove()\n",
    "        \n",
    "        hook_handlers = []\n",
    "    \n",
    "        # Normalizing the gradient to binary in {0, 1}\n",
    "        gradient = torch.ge(inputs.grad.data, 0)\n",
    "        gradient = (gradient.float() - 0.5) * 2\n",
    "        \n",
    "        # Normalizing the gradient to the same space of image\n",
    "        gradient[0][0] = (gradient[0][0]) / std[0]\n",
    "        gradient[0][1] = (gradient[0][1]) / std[1]\n",
    "        gradient[0][2] = (gradient[0][2]) / std[2]\n",
    "        \n",
    "        # Adding small perturbations to images\n",
    "        tempInputs = torch.add(inputs.detach(), gradient, alpha=-magnitude)\n",
    "        \n",
    "        if second_forward_fi:\n",
    "            hook_handler = fs.onlineMultiBitLayerOutputInjection(model=model, targetLayer=layer_name, NofError=flip_ratio, bit_positions=bit_lists)\n",
    "            hook_handlers.append(hook_handler)\n",
    "            # print(f\"[OOD] 두번째 forward pass - 레이어 {layer_name}의 {str(bit_lists)} 비트에 {flip_ratio} 비율로 결함 주입 설정\")\n",
    "            \n",
    "            if hasattr(fs, '_current_injection_stats'):\n",
    "                stats = fs._current_injection_stats\n",
    "                # print(f\"[OOD] 오류 주입된 뉴런 비율: {stats['injected_neurons']}/{stats['total_neurons']} = {stats['ratio']:.4f}\")            \n",
    "        \n",
    "        outputs = model(tempInputs)      \n",
    "        \n",
    "        for handler in hook_handlers:\n",
    "            handler.remove()    \n",
    "\n",
    "        outputs = outputs / temperature\n",
    "        \n",
    "        # Calculating the confidence after adding perturbations\n",
    "        nnOutputs = outputs.data.cpu().numpy()\n",
    "        nnOutputs = np.exp(nnOutputs - np.max(nnOutputs, axis=1, keepdims=True))\n",
    "        nnOutputs = nnOutputs / np.sum(nnOutputs, axis=1, keepdims=True)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            g2.write(\"{}, {}, {}\\n\".format(temperature, magnitude, np.max(nnOutputs[i])))\n",
    "            \n",
    "        images.grad = None\n",
    "        \n",
    "    g1.close()\n",
    "    g2.close()      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be624db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fault_injector as fi\n",
    "\n",
    "def odin_fi_ash(model=None, id_testloader=None, out_testloader=None, magnitude=0.0014, temperature=1000, std=(0,0,0), file_path='', \n",
    "            first_forward_fi=False, backward_fi=False, second_forward_fi=False, bit_lists=[], flip_ratio=0, layer_name='', fs=None, ash_method=''):    \n",
    "    g1 = open(f\"{file_path}/confidence_Our_In_fi_ash.txt\", 'w')\n",
    "    g2 = open(f\"{file_path}/confidence_Our_Out_fi_ash.txt\", 'w')\n",
    "    \n",
    "    print(\"Processing in-distribution images\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "########################################In-distribution###########################################\n",
    "    for j, data in enumerate(id_testloader):\n",
    "        images, _ = data\n",
    "        batch_size = images.size(0)\n",
    "        \n",
    "        inputs = images.to(device).requires_grad_()     \n",
    "        \n",
    "        hook_handlers = []\n",
    "        \n",
    "        if first_forward_fi:\n",
    "            hook_handler = fs.onlineMultiBitLayerOutputInjection(model=model, targetLayer=layer_name, NofError=flip_ratio, bit_positions=bit_lists)\n",
    "            hook_handlers.append(hook_handler)\n",
    "            # print(f\"[ID] 첫번째 forward pass - 레이어 {layer_name}의 {str(bit_lists)} 비트에 {flip_ratio} 비율로 결함 주입 설정\")\n",
    "            \n",
    "            if hasattr(fs, '_current_injection_stats'):\n",
    "                stats = fs._current_injection_stats\n",
    "                # print(f\"[ID] 오류 주입된 뉴런 비율: {stats['injected_neurons']}/{stats['total_neurons']} = {stats['ratio']:.4f}\")\n",
    "            \n",
    "        outputs = model.forward_ash(inputs, ash_method)              \n",
    "        \n",
    "        for handler in hook_handlers:\n",
    "            handler.remove()\n",
    "            \n",
    "        hook_handlers = []\n",
    "        \n",
    "        # Calculating the confidence of the output, no perturbation added here, no temperature scaling used\n",
    "        nnOutputs = outputs.detach().cpu().numpy()\n",
    "        nnOutputs = np.exp(nnOutputs - np.max(nnOutputs, axis=1, keepdims=True))\n",
    "        nnOutputs = nnOutputs / np.sum(nnOutputs, axis=1, keepdims=True)\n",
    "        \n",
    "        # Using temperature scaling\n",
    "        outputs = outputs / temperature\n",
    "\t\n",
    "        # Calculating the perturbation we need to add, that is,\n",
    "        # the sign of gradient of cross entropy loss w.r.t. input\n",
    "        maxIndexTemp = torch.argmax(outputs, dim=1)\n",
    "        labels = maxIndexTemp.to(device)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        if backward_fi:\n",
    "            hook_handler = fs.onlineMultiBitLayerBackwardInjection(model=model, targetLayer=layer_name, NofError=flip_ratio, bit_positions=bit_lists)\n",
    "            hook_handlers.append(hook_handler)\n",
    "            # print(f\"[ID] Backward pass - 레이어 {layer_name}의 {str(bit_lists)} 비트에 {flip_ratio} 비율로 결함 주입 설정\")\n",
    "            \n",
    "            if hasattr(fs, '_current_injection_stats'):\n",
    "                stats = fs._current_injection_stats\n",
    "                # print(f\"[ID] 오류 주입된 뉴런 비율: {stats['injected_neurons']}/{stats['total_neurons']} = {stats['ratio']:.4f}\")\n",
    "            \n",
    "        loss.backward()\n",
    "        \n",
    "        for handler in hook_handlers:\n",
    "            handler.remove()\n",
    "        \n",
    "        hook_handlers = []\n",
    "                    \n",
    "        # Normalizing the gradient to binary in {0, 1}\n",
    "        gradient = torch.ge(inputs.grad.data, 0)\n",
    "        gradient = (gradient.float() - 0.5) * 2\n",
    "        \n",
    "        # Normalizing the gradient to the same space of image\n",
    "        gradient[0][0] = (gradient[0][0]) / std[0]\n",
    "        gradient[0][1] = (gradient[0][1]) / std[1]\n",
    "        gradient[0][2] = (gradient[0][2]) / std[2]\n",
    "        \n",
    "        # Adding small perturbations to images\n",
    "        tempInputs = torch.add(inputs.detach(), gradient, alpha=-magnitude)\n",
    "        \n",
    "        if second_forward_fi:\n",
    "            hook_handler = fs.onlineMultiBitLayerOutputInjection(model=model, targetLayer=layer_name, NofError=flip_ratio, bit_positions=bit_lists)\n",
    "            hook_handlers.append(hook_handler)\n",
    "            # print(f\"[ID] 두번째 forward pass - 레이어 {layer_name}의 {str(bit_lists)} 비트에 {flip_ratio} 비율로 결함 주입 설정\")\n",
    "            \n",
    "            if hasattr(fs, '_current_injection_stats'):\n",
    "                stats = fs._current_injection_stats\n",
    "                # print(f\"[ID] 오류 주입된 뉴런 비율: {stats['injected_neurons']}/{stats['total_neurons']} = {stats['ratio']:.4f}\")            \n",
    "        \n",
    "        outputs = model.forward_ash(tempInputs, ash_method)       \n",
    "        \n",
    "        for handler in hook_handlers:\n",
    "            handler.remove()       \n",
    "\n",
    "        outputs = outputs / temperature\n",
    "        \n",
    "        # Calculating the confidence after adding perturbations\n",
    "        nnOutputs = outputs.data.cpu().numpy()\n",
    "        nnOutputs = np.exp(nnOutputs - np.max(nnOutputs, axis=1, keepdims=True))\n",
    "        nnOutputs = nnOutputs / np.sum(nnOutputs, axis=1, keepdims=True)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            g1.write(\"{}, {}, {}\\n\".format(temperature, magnitude, np.max(nnOutputs[i])))\n",
    "            \n",
    "        images.grad = None\n",
    "        \n",
    "    print(\"Processing out-of-distribution images\")\n",
    "###################################Out-of-Distributions#####################################\n",
    "    for j, data in enumerate(out_testloader):\n",
    "        images, _ = data\n",
    "        batch_size = images.size(0)\n",
    "    \n",
    "        inputs = images.to(device).requires_grad_()           \n",
    "            \n",
    "        hook_handlers = []\n",
    "        \n",
    "        if first_forward_fi:\n",
    "            hook_handler = fs.onlineMultiBitLayerOutputInjection(model=model, targetLayer=layer_name, NofError=flip_ratio, bit_positions=bit_lists)\n",
    "            hook_handlers.append(hook_handler)\n",
    "            # print(f\"[OOD] 첫번째 forward pass - 레이어 {layer_name}의 {str(bit_lists)} 비트에 {flip_ratio} 비율로 결함 주입 설정\")\n",
    "            \n",
    "            if hasattr(fs, '_current_injection_stats'):\n",
    "                stats = fs._current_injection_stats\n",
    "                # print(f\"[OOD] 오류 주입된 뉴런 비율: {stats['injected_neurons']}/{stats['total_neurons']} = {stats['ratio']:.4f}\")            \n",
    "            \n",
    "        outputs = model.forward_ash(inputs, ash_method)           \n",
    "        \n",
    "        for handler in hook_handlers:\n",
    "            handler.remove()\n",
    "            \n",
    "        hook_handlers = []\n",
    "        \n",
    "        # Calculating the confidence of the output, no perturbation added here\n",
    "        nnOutputs = outputs.detach().cpu().numpy()\n",
    "        nnOutputs = np.exp(nnOutputs - np.max(nnOutputs, axis=1, keepdims=True))\n",
    "        nnOutputs = nnOutputs / np.sum(nnOutputs, axis=1, keepdims=True)\n",
    "        \n",
    "        # Using temperature scaling\n",
    "        outputs = outputs / temperature  \n",
    "  \n",
    "        # Calculating the perturbation we need to add, that is,\n",
    "        # the sign of gradient of cross entropy loss w.r.t. input\n",
    "        maxIndexTemp = torch.argmax(outputs, dim=1)\n",
    "        labels = maxIndexTemp.to(device)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        if backward_fi:\n",
    "            hook_handler = fs.onlineMultiBitLayerBackwardInjection(model=model, targetLayer=layer_name, NofError=flip_ratio, bit_positions=bit_lists)\n",
    "            hook_handlers.append(hook_handler)\n",
    "            # print(f\"[OOD] Backward pass - 레이어 {layer_name}의 {str(bit_lists)} 비트에 {flip_ratio} 비율로 결함 주입 설정\")\n",
    "            \n",
    "            if hasattr(fs, '_current_injection_stats'):\n",
    "                stats = fs._current_injection_stats\n",
    "                # print(f\"[OOD] 오류 주입된 뉴런 비율: {stats['injected_neurons']}/{stats['total_neurons']} = {stats['ratio']:.4f}\")            \n",
    "            \n",
    "        loss.backward()\n",
    "        \n",
    "        for handler in hook_handlers:\n",
    "            handler.remove()\n",
    "        \n",
    "        hook_handlers = []\n",
    "    \n",
    "        # Normalizing the gradient to binary in {0, 1}\n",
    "        gradient = torch.ge(inputs.grad.data, 0)\n",
    "        gradient = (gradient.float() - 0.5) * 2\n",
    "        \n",
    "        # Normalizing the gradient to the same space of image\n",
    "        gradient[0][0] = (gradient[0][0]) / std[0]\n",
    "        gradient[0][1] = (gradient[0][1]) / std[1]\n",
    "        gradient[0][2] = (gradient[0][2]) / std[2]\n",
    "        \n",
    "        # Adding small perturbations to images\n",
    "        tempInputs = torch.add(inputs.detach(), gradient, alpha=-magnitude)\n",
    "        \n",
    "        if second_forward_fi:\n",
    "            hook_handler = fs.onlineMultiBitLayerOutputInjection(model=model, targetLayer=layer_name, NofError=flip_ratio, bit_positions=bit_lists)\n",
    "            hook_handlers.append(hook_handler)\n",
    "            # print(f\"[OOD] 두번째 forward pass - 레이어 {layer_name}의 {str(bit_lists)} 비트에 {flip_ratio} 비율로 결함 주입 설정\")\n",
    "            \n",
    "            if hasattr(fs, '_current_injection_stats'):\n",
    "                stats = fs._current_injection_stats\n",
    "                # print(f\"[OOD] 오류 주입된 뉴런 비율: {stats['injected_neurons']}/{stats['total_neurons']} = {stats['ratio']:.4f}\")            \n",
    "        \n",
    "        outputs = model.forward_ash(tempInputs, ash_method)       \n",
    "        \n",
    "        for handler in hook_handlers:\n",
    "            handler.remove()    \n",
    "\n",
    "        outputs = outputs / temperature\n",
    "        \n",
    "        # Calculating the confidence after adding perturbations\n",
    "        nnOutputs = outputs.data.cpu().numpy()\n",
    "        nnOutputs = np.exp(nnOutputs - np.max(nnOutputs, axis=1, keepdims=True))\n",
    "        nnOutputs = nnOutputs / np.sum(nnOutputs, axis=1, keepdims=True)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            g2.write(\"{}, {}, {}\\n\".format(temperature, magnitude, np.max(nnOutputs[i])))\n",
    "            \n",
    "        images.grad = None\n",
    "        \n",
    "    g1.close()\n",
    "    g2.close()      \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
