{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d607a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11e974a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d7ecc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation_bitflip_hook(bit_positions, flip_ratio, flip_log):\n",
    "    if isinstance(bit_positions, int):\n",
    "        bit_positions = [bit_positions]\n",
    "        \n",
    "    def hook(module, inp, out):\n",
    "        with torch.no_grad():\n",
    "            # out: torch.Tensor\n",
    "            flat = out.view(-1)\n",
    "            total = flat.numel()\n",
    "            num_flips = max(1, int(total * flip_ratio))\n",
    "            # 랜덤으로 num_flips개 인덱스 선택\n",
    "            idxs = torch.randperm(total, device=flat.device)[:num_flips]\n",
    "            for idx in idxs:\n",
    "                orig = flat[idx].item()\n",
    "                # float32 비트 패턴→uint32\n",
    "                i = np.frombuffer(np.float32(orig).tobytes(), dtype=np.uint32)[0]\n",
    "                # bit flip\n",
    "                for pos in bit_positions:\n",
    "                    i ^= (1 << pos)\n",
    "                # 다시 float32\n",
    "                flipped = np.frombuffer(np.uint32(i).tobytes(), dtype=np.float32)[0]\n",
    "                flat[idx] = torch.as_tensor(flipped, dtype=flat.dtype, device=flat.device)\n",
    "                flip_log.append((idx.item(), orig, flipped, bit_positions.copy()))\n",
    "        return out\n",
    "    return hook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0a0cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_bitflip_hook(bit_positions, flip_ratio, flip_log):\n",
    "    if isinstance(bit_positions, int):\n",
    "        bit_positions = [bit_positions]\n",
    "        \n",
    "    def hook(grad):\n",
    "        with torch.no_grad():\n",
    "            flat = grad.view(-1)\n",
    "            total = flat.numel()\n",
    "            num_flips = max(1, int(total * flip_ratio))\n",
    "            idxs = torch.randperm(flat.numel(), device=flat.device)[:num_flips]\n",
    "            for idx in idxs:\n",
    "                orig = flat[idx].item()\n",
    "                # float32 → uint32 비트 패턴\n",
    "                i = np.frombuffer(np.float32(orig).tobytes(), dtype=np.uint32)[0]\n",
    "                # 지정한 위치 비트 flip\n",
    "                for pos in bit_positions:\n",
    "                    i ^= (1 << pos)\n",
    "                # 다시 float32\n",
    "                flipped = np.frombuffer(np.uint32(i).tobytes(), dtype=np.float32)[0]\n",
    "                flat[idx] = torch.as_tensor(flipped, dtype=flat.dtype, device=flat.device)\n",
    "                flip_log.append((idx.item(), orig, flipped, bit_positions.copy()))\n",
    "        # 원래 shape으로 복원\n",
    "        return grad\n",
    "    return hook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7b0d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_name(model):\n",
    "    layer_names = []\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if any(keyword in name.lower() for keyword in ['conv', 'linear', 'fc']):\n",
    "            layer_names.append(name)\n",
    "    \n",
    "    if not layer_names:\n",
    "        return None  # 혹시 찾은 레이어가 없으면 None 반환\n",
    "\n",
    "    return layer_names"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
