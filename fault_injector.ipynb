{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d607a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11e974a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d7ecc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation_bitflip_hook(bit_position, num_flips, flip_log):\n",
    "    \"\"\"\n",
    "    forward 시 출력된 activation tensor에 대해\n",
    "    num_flips개 위치의 bit_position 비트를 뒤집습니다.\n",
    "    \"\"\"\n",
    "    def hook(module, inp, out):\n",
    "        # out: torch.Tensor\n",
    "        flat = out.view(-1)\n",
    "        total = flat.numel()\n",
    "        # 랜덤으로 num_flips개 인덱스 선택\n",
    "        idxs = torch.randperm(total, device=flat.device)[:num_flips]\n",
    "        for idx in idxs:\n",
    "            orig = flat[idx].item()\n",
    "            # float32 비트 패턴→uint32\n",
    "            i = np.frombuffer(np.float32(orig).tobytes(), dtype=np.uint32)[0]\n",
    "            # bit flip\n",
    "            i ^= (1 << bit_position)\n",
    "            # 다시 float32\n",
    "            flipped = np.frombuffer(np.uint32(i).tobytes(), dtype=np.float32)[0]\n",
    "            flat[idx] = torch.tensor(flipped, device=flat.device)\n",
    "            flip_log.append((idx.item(), orig, flipped))\n",
    "        return flat.view_as(out)\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0a0cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_bitflip_hook(bit_position, num_flips, flip_log):\n",
    "    \"\"\"\n",
    "    activation.register_hook()에 넘길 backward‐gradeint용 hook.\n",
    "    들어온 grad tensor의 num_flips개 위치의 bit_position 비트를 뒤집고 반환합니다.\n",
    "    \"\"\"\n",
    "    def hook(grad):\n",
    "        flat = grad.view(-1)\n",
    "        idxs = torch.randperm(flat.numel(), device=flat.device)[:num_flips]\n",
    "        for idx in idxs:\n",
    "            orig = flat[idx].item()\n",
    "            # float32 → uint32 비트 패턴\n",
    "            i = np.frombuffer(np.float32(orig).tobytes(), dtype=np.uint32)[0]\n",
    "            # 지정한 위치 비트 flip\n",
    "            i ^= (1 << bit_position)\n",
    "            # 다시 float32\n",
    "            flipped = np.frombuffer(np.uint32(i).tobytes(), dtype=np.float32)[0]\n",
    "            flat[idx] = torch.tensor(flipped, device=flat.device)\n",
    "            flip_log.append((idx.item(), orig, flipped))\n",
    "        # 원래 shape으로 복원\n",
    "        return flat.view_as(grad)\n",
    "    return hook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7b0d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_name(model):\n",
    "    layer_names = []\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if any(keyword in name.lower() for keyword in ['conv', 'linear', 'fc']):\n",
    "            layer_names.append(name)\n",
    "    \n",
    "    if not layer_names:\n",
    "        return None  # 혹시 찾은 레이어가 없으면 None 반환\n",
    "\n",
    "    return layer_names"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
