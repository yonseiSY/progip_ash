{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78314e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Copyright (c) 2017-present, Facebook, Inc.\n",
    "# All rights reserved.\n",
    "#\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "#\n",
    "\n",
    "\"\"\"\n",
    "Created on Sat Sep 19 20:55:56 2015\n",
    "\n",
    "@author: liangshiyu\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy import misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e436a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testData(net1, criterion, CUDA_DEVICE, testloader10, testloader, nnName, dataName, noiseMagnitude1, temper):\n",
    "    t0 = time.time()\n",
    "    f1 = open(\"./softmax_scores/confidence_Base_In.txt\", 'w')\n",
    "    f2 = open(\"./softmax_scores/confidence_Base_Out.txt\", 'w')\n",
    "    g1 = open(\"./softmax_scores/confidence_Our_In.txt\", 'w')\n",
    "    g2 = open(\"./softmax_scores/confidence_Our_Out.txt\", 'w')\n",
    "    N = 10000\n",
    "    if dataName == \"iSUN\": N = 8925\n",
    "    print(\"Processing in-distribution images\")\n",
    "########################################In-distribution###########################################\n",
    "    for j, data in enumerate(testloader10):\n",
    "        if j<1000: continue\n",
    "        images, _ = data\n",
    "        \n",
    "        inputs = Variable(images.cuda(CUDA_DEVICE), requires_grad = True)\n",
    "        outputs = net1(inputs)\n",
    "        \n",
    "\n",
    "        # Calculating the confidence of the output, no perturbation added here, no temperature scaling used\n",
    "        nnOutputs = outputs.data.cpu()\n",
    "        nnOutputs = nnOutputs.numpy()\n",
    "        nnOutputs = nnOutputs[0]\n",
    "        nnOutputs = nnOutputs - np.max(nnOutputs)\n",
    "        nnOutputs = np.exp(nnOutputs)/np.sum(np.exp(nnOutputs))\n",
    "        f1.write(\"{}, {}, {}\\n\".format(temper, noiseMagnitude1, np.max(nnOutputs)))\n",
    "        \n",
    "        # Using temperature scaling\n",
    "        outputs = outputs / temper\n",
    "\t\n",
    "        # Calculating the perturbation we need to add, that is,\n",
    "        # the sign of gradient of cross entropy loss w.r.t. input\n",
    "        maxIndexTemp = np.argmax(nnOutputs)\n",
    "        labels = Variable(torch.LongTensor([maxIndexTemp]).cuda(CUDA_DEVICE))\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Normalizing the gradient to binary in {0, 1}\n",
    "        gradient =  torch.ge(inputs.grad.data, 0)\n",
    "        gradient = (gradient.float() - 0.5) * 2\n",
    "        # Normalizing the gradient to the same space of image\n",
    "        gradient[0][0] = (gradient[0][0] )/(63.0/255.0)\n",
    "        gradient[0][1] = (gradient[0][1] )/(62.1/255.0)\n",
    "        gradient[0][2] = (gradient[0][2])/(66.7/255.0)\n",
    "        # Adding small perturbations to images\n",
    "        tempInputs = torch.add(inputs.data,  -noiseMagnitude1, gradient)\n",
    "        outputs = net1(Variable(tempInputs))\n",
    "        outputs = outputs / temper\n",
    "        # Calculating the confidence after adding perturbations\n",
    "        nnOutputs = outputs.data.cpu()\n",
    "        nnOutputs = nnOutputs.numpy()\n",
    "        nnOutputs = nnOutputs[0]\n",
    "        nnOutputs = nnOutputs - np.max(nnOutputs)\n",
    "        nnOutputs = np.exp(nnOutputs)/np.sum(np.exp(nnOutputs))\n",
    "        g1.write(\"{}, {}, {}\\n\".format(temper, noiseMagnitude1, np.max(nnOutputs)))\n",
    "        if j % 100 == 99:\n",
    "            print(\"{:4}/{:4} images processed, {:.1f} seconds used.\".format(j+1-1000, N-1000, time.time()-t0))\n",
    "            t0 = time.time()\n",
    "        \n",
    "        if j == N - 1: break\n",
    "\n",
    "\n",
    "    t0 = time.time()\n",
    "    print(\"Processing out-of-distribution images\")\n",
    "###################################Out-of-Distributions#####################################\n",
    "    for j, data in enumerate(testloader):\n",
    "        if j<1000: continue\n",
    "        images, _ = data\n",
    "    \n",
    "    \n",
    "        inputs = Variable(images.cuda(CUDA_DEVICE), requires_grad = True)\n",
    "        outputs = net1(inputs)\n",
    "        \n",
    "\n",
    "\n",
    "        # Calculating the confidence of the output, no perturbation added here\n",
    "        nnOutputs = outputs.data.cpu()\n",
    "        nnOutputs = nnOutputs.numpy()\n",
    "        nnOutputs = nnOutputs[0]\n",
    "        nnOutputs = nnOutputs - np.max(nnOutputs)\n",
    "        nnOutputs = np.exp(nnOutputs)/np.sum(np.exp(nnOutputs))\n",
    "        f2.write(\"{}, {}, {}\\n\".format(temper, noiseMagnitude1, np.max(nnOutputs)))\n",
    "        \n",
    "        # Using temperature scaling\n",
    "        outputs = outputs / temper\n",
    "  \n",
    "  \n",
    "        # Calculating the perturbation we need to add, that is,\n",
    "        # the sign of gradient of cross entropy loss w.r.t. input\n",
    "        maxIndexTemp = np.argmax(nnOutputs)\n",
    "        labels = Variable(torch.LongTensor([maxIndexTemp]).cuda(CUDA_DEVICE))\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Normalizing the gradient to binary in {0, 1}\n",
    "        gradient =  (torch.ge(inputs.grad.data, 0))\n",
    "        gradient = (gradient.float() - 0.5) * 2\n",
    "        # Normalizing the gradient to the same space of image\n",
    "        gradient[0][0] = (gradient[0][0] )/(63.0/255.0)\n",
    "        gradient[0][1] = (gradient[0][1] )/(62.1/255.0)\n",
    "        gradient[0][2] = (gradient[0][2])/(66.7/255.0)\n",
    "        # Adding small perturbations to images\n",
    "        tempInputs = torch.add(inputs.data,  -noiseMagnitude1, gradient)\n",
    "        outputs = net1(Variable(tempInputs))\n",
    "        outputs = outputs / temper\n",
    "        # Calculating the confidence after adding perturbations\n",
    "        nnOutputs = outputs.data.cpu()\n",
    "        nnOutputs = nnOutputs.numpy()\n",
    "        nnOutputs = nnOutputs[0]\n",
    "        nnOutputs = nnOutputs - np.max(nnOutputs)\n",
    "        nnOutputs = np.exp(nnOutputs)/np.sum(np.exp(nnOutputs))\n",
    "        g2.write(\"{}, {}, {}\\n\".format(temper, noiseMagnitude1, np.max(nnOutputs)))\n",
    "        if j % 100 == 99:\n",
    "            print(\"{:4}/{:4} images processed, {:.1f} seconds used.\".format(j+1-1000, N-1000, time.time()-t0))\n",
    "            t0 = time.time()\n",
    "\n",
    "        if j== N-1: break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def testGaussian(net1, criterion, CUDA_DEVICE, testloader10, testloader, nnName, dataName, noiseMagnitude1, temper):\n",
    "    t0 = time.time()\n",
    "    f1 = open(\"./softmax_scores/confidence_Base_In.txt\", 'w')\n",
    "    f2 = open(\"./softmax_scores/confidence_Base_Out.txt\", 'w')\n",
    "    g1 = open(\"./softmax_scores/confidence_Our_In.txt\", 'w')\n",
    "    g2 = open(\"./softmax_scores/confidence_Our_Out.txt\", 'w')\n",
    "########################################In-Distribution###############################################\n",
    "    N = 10000\n",
    "    print(\"Processing in-distribution images\")\n",
    "    for j, data in enumerate(testloader10):\n",
    "        \n",
    "        if j<1000: continue\n",
    "        images, _ = data\n",
    "        \n",
    "        inputs = Variable(images.cuda(CUDA_DEVICE), requires_grad = True)\n",
    "        outputs = net1(inputs)\n",
    "        \n",
    "        \n",
    "        # Calculating the confidence of the output, no perturbation added here\n",
    "        nnOutputs = outputs.data.cpu()\n",
    "        nnOutputs = nnOutputs.numpy()\n",
    "        nnOutputs = nnOutputs[0]\n",
    "        nnOutputs = nnOutputs - np.max(nnOutputs)\n",
    "        nnOutputs = np.exp(nnOutputs)/np.sum(np.exp(nnOutputs))\n",
    "        f1.write(\"{}, {}, {}\\n\".format(temper, noiseMagnitude1, np.max(nnOutputs)))\n",
    "        \n",
    "        # Using temperature scaling\n",
    "        outputs = outputs / temper\n",
    "        \n",
    "        # Calculating the perturbation we need to add, that is,\n",
    "        # the sign of gradient of cross entropy loss w.r.t. input\n",
    "        maxIndexTemp = np.argmax(nnOutputs)\n",
    "        labels = Variable(torch.LongTensor([maxIndexTemp]).cuda(CUDA_DEVICE))\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        \n",
    "        # Normalizing the gradient to binary in {0, 1}\n",
    "        gradient =  (torch.ge(inputs.grad.data, 0))\n",
    "        gradient = (gradient.float() - 0.5) * 2\n",
    "        # Normalizing the gradient to the same space of image\n",
    "        gradient[0][0] = (gradient[0][0] )/(63.0/255.0)\n",
    "        gradient[0][1] = (gradient[0][1] )/(62.1/255.0)\n",
    "        gradient[0][2] = (gradient[0][2])/(66.7/255.0)\n",
    "        # Adding small perturbations to images\n",
    "        tempInputs = torch.add(inputs.data,  -noiseMagnitude1, gradient)\n",
    "        outputs = net1(Variable(tempInputs))\n",
    "        outputs = outputs / temper\n",
    "        # Calculating the confidence after adding perturbations\n",
    "        nnOutputs = outputs.data.cpu()\n",
    "        nnOutputs = nnOutputs.numpy()\n",
    "        nnOutputs = nnOutputs[0]\n",
    "        nnOutputs = nnOutputs - np.max(nnOutputs)\n",
    "        nnOutputs = np.exp(nnOutputs)/np.sum(np.exp(nnOutputs))\n",
    "\n",
    "        g1.write(\"{}, {}, {}\\n\".format(temper, noiseMagnitude1, np.max(nnOutputs)))\n",
    "        if j % 100 == 99:\n",
    "            print(\"{:4}/{:4} images processed, {:.1f} seconds used.\".format(j+1-1000, N-1000, time.time()-t0))\n",
    "            t0 = time.time()\n",
    "\n",
    "    \n",
    "    \n",
    "########################################Out-of-Distribution######################################\n",
    "    print(\"Processing out-of-distribution images\")\n",
    "    for j, data in enumerate(testloader):\n",
    "        if j<1000: continue\n",
    "        \n",
    "        images = torch.randn(1,3,32,32) + 0.5\n",
    "        images = torch.clamp(images, 0, 1)\n",
    "        images[0][0] = (images[0][0] - 125.3/255) / (63.0/255)\n",
    "        images[0][1] = (images[0][1] - 123.0/255) / (62.1/255)\n",
    "        images[0][2] = (images[0][2] - 113.9/255) / (66.7/255)\n",
    "        \n",
    "        \n",
    "        inputs = Variable(images.cuda(CUDA_DEVICE), requires_grad = True)\n",
    "        outputs = net1(inputs)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Calculating the confidence of the output, no perturbation added here\n",
    "        nnOutputs = outputs.data.cpu()\n",
    "        nnOutputs = nnOutputs.numpy()\n",
    "        nnOutputs = nnOutputs[0]\n",
    "        nnOutputs = nnOutputs - np.max(nnOutputs)\n",
    "        nnOutputs = np.exp(nnOutputs)/np.sum(np.exp(nnOutputs))\n",
    "        f2.write(\"{}, {}, {}\\n\".format(temper, noiseMagnitude1, np.max(nnOutputs)))\n",
    "        \n",
    "        # Using temperature scaling\n",
    "        outputs = outputs / temper\n",
    "        \n",
    "        # Calculating the perturbation we need to add, that is,\n",
    "        # the sign of gradient of cross entropy loss w.r.t. input\n",
    "        maxIndexTemp = np.argmax(nnOutputs)\n",
    "        labels = Variable(torch.LongTensor([maxIndexTemp]).cuda(CUDA_DEVICE))\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Normalizing the gradient to binary in {0, 1}\n",
    "        gradient =  (torch.ge(inputs.grad.data, 0))\n",
    "        gradient = (gradient.float() - 0.5) * 2\n",
    "        # Normalizing the gradient to the same space of image\n",
    "        gradient[0][0] = (gradient[0][0] )/(63.0/255.0)\n",
    "        gradient[0][1] = (gradient[0][1] )/(62.1/255.0)\n",
    "        gradient[0][2] = (gradient[0][2])/(66.7/255.0)\n",
    "        # Adding small perturbations to images\n",
    "        tempInputs = torch.add(inputs.data,  -noiseMagnitude1, gradient)\n",
    "        outputs = net1(Variable(tempInputs))\n",
    "        outputs = outputs / temper\n",
    "        # Calculating the confidence after adding perturbations\n",
    "        nnOutputs = outputs.data.cpu()\n",
    "        nnOutputs = nnOutputs.numpy()\n",
    "        nnOutputs = nnOutputs[0]\n",
    "        nnOutputs = nnOutputs - np.max(nnOutputs)\n",
    "        nnOutputs = np.exp(nnOutputs)/np.sum(np.exp(nnOutputs))\n",
    "        g2.write(\"{}, {}, {}\\n\".format(temper, noiseMagnitude1, np.max(nnOutputs)))\n",
    "        \n",
    "        if j % 100 == 99:\n",
    "            print(\"{:4}/{:4} images processed, {:.1f} seconds used.\".format(j+1-1000, N-1000, time.time()-t0))\n",
    "            t0 = time.time()\n",
    "\n",
    "        if j== N-1: break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def testUni(net1, criterion, CUDA_DEVICE, testloader10, testloader, nnName, dataName, noiseMagnitude1, temper):\n",
    "    t0 = time.time()\n",
    "    f1 = open(\"./softmax_scores/confidence_Base_In.txt\", 'w')\n",
    "    f2 = open(\"./softmax_scores/confidence_Base_Out.txt\", 'w')\n",
    "    g1 = open(\"./softmax_scores/confidence_Our_In.txt\", 'w')\n",
    "    g2 = open(\"./softmax_scores/confidence_Our_Out.txt\", 'w')\n",
    "########################################In-Distribution###############################################\n",
    "    N = 10000\n",
    "    print(\"Processing in-distribution images\")\n",
    "    for j, data in enumerate(testloader10):\n",
    "        if j<1000: continue\n",
    "        \n",
    "        images, _ = data\n",
    "        \n",
    "        inputs = Variable(images.cuda(CUDA_DEVICE), requires_grad = True)\n",
    "        outputs = net1(inputs)\n",
    "        \n",
    "        \n",
    "        # Calculating the confidence of the output, no perturbation added here\n",
    "        nnOutputs = outputs.data.cpu()\n",
    "        nnOutputs = nnOutputs.numpy()\n",
    "        nnOutputs = nnOutputs[0]\n",
    "        nnOutputs = nnOutputs - np.max(nnOutputs)\n",
    "        nnOutputs = np.exp(nnOutputs)/np.sum(np.exp(nnOutputs))\n",
    "        f1.write(\"{}, {}, {}\\n\".format(temper, noiseMagnitude1, np.max(nnOutputs)))\n",
    "        \n",
    "        # Using temperature scaling\n",
    "        outputs = outputs / temper\n",
    "        \n",
    "        # Calculating the perturbation we need to add, that is,\n",
    "        # the sign of gradient of cross entropy loss w.r.t. input\n",
    "        maxIndexTemp = np.argmax(nnOutputs)\n",
    "        labels = Variable(torch.LongTensor([maxIndexTemp]).cuda(CUDA_DEVICE))\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        \n",
    "        # Normalizing the gradient to binary in {0, 1}\n",
    "        gradient =  (torch.ge(inputs.grad.data, 0))\n",
    "        gradient = (gradient.float() - 0.5) * 2\n",
    "        # Normalizing the gradient to the same space of image\n",
    "        gradient[0][0] = (gradient[0][0] )/(63.0/255.0)\n",
    "        gradient[0][1] = (gradient[0][1] )/(62.1/255.0)\n",
    "        gradient[0][2] = (gradient[0][2])/(66.7/255.0)\n",
    "        # Adding small perturbations to images\n",
    "        tempInputs = torch.add(inputs.data,  -noiseMagnitude1, gradient)\n",
    "        outputs = net1(Variable(tempInputs))\n",
    "        outputs = outputs / temper\n",
    "        # Calculating the confidence after adding perturbations\n",
    "        nnOutputs = outputs.data.cpu()\n",
    "        nnOutputs = nnOutputs.numpy()\n",
    "        nnOutputs = nnOutputs[0]\n",
    "        nnOutputs = nnOutputs - np.max(nnOutputs)\n",
    "        nnOutputs = np.exp(nnOutputs)/np.sum(np.exp(nnOutputs))\n",
    "\n",
    "        g1.write(\"{}, {}, {}\\n\".format(temper, noiseMagnitude1, np.max(nnOutputs)))\n",
    "        if j % 100 == 99:\n",
    "            print(\"{:4}/{:4}  images processed, {:.1f} seconds used.\".format(j+1-1000, N-1000, time.time()-t0))\n",
    "            t0 = time.time()\n",
    "\n",
    "\n",
    "\n",
    "########################################Out-of-Distribution######################################\n",
    "    print(\"Processing out-of-distribution images\")\n",
    "    for j, data in enumerate(testloader):\n",
    "        if j<1000: continue\n",
    "        \n",
    "        images = torch.rand(1,3,32,32)\n",
    "        images[0][0] = (images[0][0] - 125.3/255) / (63.0/255)\n",
    "        images[0][1] = (images[0][1] - 123.0/255) / (62.1/255)\n",
    "        images[0][2] = (images[0][2] - 113.9/255) / (66.7/255)\n",
    "        \n",
    "        \n",
    "        inputs = Variable(images.cuda(CUDA_DEVICE), requires_grad = True)\n",
    "        outputs = net1(inputs)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Calculating the confidence of the output, no perturbation added here\n",
    "        nnOutputs = outputs.data.cpu()\n",
    "        nnOutputs = nnOutputs.numpy()\n",
    "        nnOutputs = nnOutputs[0]\n",
    "        nnOutputs = nnOutputs - np.max(nnOutputs)\n",
    "        nnOutputs = np.exp(nnOutputs)/np.sum(np.exp(nnOutputs))\n",
    "        f2.write(\"{}, {}, {}\\n\".format(temper, noiseMagnitude1, np.max(nnOutputs)))\n",
    "        \n",
    "        # Using temperature scaling\n",
    "        outputs = outputs / temper\n",
    "        \n",
    "        # Calculating the perturbation we need to add, that is,\n",
    "        # the sign of gradient of cross entropy loss w.r.t. input\n",
    "        maxIndexTemp = np.argmax(nnOutputs)\n",
    "        labels = Variable(torch.LongTensor([maxIndexTemp]).cuda(CUDA_DEVICE))\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Normalizing the gradient to binary in {0, 1}\n",
    "        gradient =  (torch.ge(inputs.grad.data, 0))\n",
    "        gradient = (gradient.float() - 0.5) * 2\n",
    "        # Normalizing the gradient to the same space of image\n",
    "        gradient[0][0] = (gradient[0][0] )/(63.0/255.0)\n",
    "        gradient[0][1] = (gradient[0][1] )/(62.1/255.0)\n",
    "        gradient[0][2] = (gradient[0][2])/(66.7/255.0)\n",
    "        # Adding small perturbations to images\n",
    "        tempInputs = torch.add(inputs.data,  -noiseMagnitude1, gradient)\n",
    "        outputs = net1(Variable(tempInputs))\n",
    "        outputs = outputs / temper\n",
    "        # Calculating the confidence after adding perturbations\n",
    "        nnOutputs = outputs.data.cpu()\n",
    "        nnOutputs = nnOutputs.numpy()\n",
    "        nnOutputs = nnOutputs[0]\n",
    "        nnOutputs = nnOutputs - np.max(nnOutputs)\n",
    "        nnOutputs = np.exp(nnOutputs)/np.sum(np.exp(nnOutputs))\n",
    "        g2.write(\"{}, {}, {}\\n\".format(temper, noiseMagnitude1, np.max(nnOutputs)))\n",
    "        if j % 100 == 99:\n",
    "            print(\"{:4}/{:4} images processed, {:.1f} seconds used.\".format(j+1-1000, N-1000, time.time()-t0))\n",
    "            t0 = time.time()\n",
    "\n",
    "        if j== N-1: break"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
